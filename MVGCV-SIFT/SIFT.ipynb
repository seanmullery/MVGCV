{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a83aeb-ca26-417c-bedc-96aa6f401053",
   "metadata": {},
   "source": [
    "![\"HCI Banner Logos for ATU Sligo, the HCI Human capital initiative and Higher Education 4.0\"](images/HCIBanner.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4a5ab-8f8c-41ad-a9d3-523e71418497",
   "metadata": {},
   "source": [
    "# SIFT\n",
    "\n",
    "SIFT - Scale Invariant Feature Transform.\n",
    "\n",
    "This is both a feature detector and descriptor and was the work of David Lowe \n",
    "[SIFT Paper](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).\n",
    "\n",
    "It is one of the most important topics in computer vision of the last two decades.\n",
    "\n",
    "However, it is patented by the University of British Columbia.\n",
    "\n",
    "There are similar methods, such as [RootSIFT](https://www.robots.ox.ac.uk/~vgg/publications/2012/Arandjelovic12/arandjelovic12.pdf), which are not.\n",
    "\n",
    "SIFT is believed to be similar to systems that evolved in Primate visual systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e57046-a521-45b0-91c7-762f8bd7dfea",
   "metadata": {},
   "source": [
    "## Goals of SIFT\n",
    "\n",
    "- Extract distinctive invariant features. i.e. it should be possible to correctly match a feature in one image against a database of many images. This makes it suitable for the wide-baseline case.\n",
    "- Feature descriptors should be invariant to scale and rotation.\n",
    "- The features should be robust to \n",
    "- Affine distortion.\n",
    "- Changes in 3D view point.\n",
    "- Addition of noise.\n",
    "- Change in Illumination\n",
    "\n",
    "Think back to the optic flow. It required brightness constancy and small changes in viewpoint.\n",
    "\n",
    "SIFT's goal was to overcome all those issues.\n",
    "\n",
    "\n",
    "## Advantages of SIFT\n",
    "\n",
    "- Features are local, so they are robust to occlusion and clutter.\n",
    "- Individual features can be matched to a large database of objects.\n",
    "- Many features can be generated for even small objects.  Typically, a $500\\times500$ image will produce approximately $2000$ stable features.\n",
    "- At the time SIFT was introduced it had near real-time performance. In the two decades that have passed since, computer hardware has increased in relation to Moore's law.\n",
    "- However embedded systems are always restricted in speed and energy usage. Whether it can run in real time is a matter for the power and resources of the hardware platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46cfdc-7cd3-4aa3-8634-13854cd0c88b",
   "metadata": {},
   "source": [
    "## Major Stages\n",
    "\n",
    "- Scale-space extrema detection: This first stage of computation searches over all scales and image locations. It is implemented efficiently by using a difference of Gaussian function to identify potential interest points that are invariant to scale and orientation.\n",
    "- Keypoint localisation: At each candidate location, a detailed model is fit to determine location and scale. Keypoints are selected based on measures of their stability.\n",
    "- Orientation and assignment: One or more orientations are assigned to each keypoint location based on local image gradient directions.  All future operations performed on image data that has been transformed relative to the assigned orientation, scale, and location for each feature, thereby providing invariance to these transformations.\n",
    "- Keypoint description: The local image gradients are measured at the selected scale in the region around each keypoint. These are transformed into a representation that allows for significant levels of local shape distortion and changes in illumination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d192bc-7076-4739-9ed5-03fa061c375a",
   "metadata": {},
   "source": [
    "##  Scale\n",
    "The problem with many feature detectors is that they are good if the feature is of just the right size.\n",
    "\n",
    "But too big or too small and they don't detect it.\n",
    "\n",
    "This is also an issue if the scale in the image changes as a feature may be found in one image but not in another, so it cannot be matched.\n",
    "\n",
    "In Canny or LoG the scale is determined by the $\\sigma$ value.\n",
    "\n",
    "If we want to look at multiple scales, we can apply the detector at each of these $\\sigma$ values but how then do we combine the results?\n",
    "\n",
    "### Earlier Work on Scale\n",
    "\n",
    "Marr-Hildreth used a spatial coincidence assumption. i.e. if the zero crossings coincided over several scales then they are physically significant.\n",
    "Andrew Witkin in his paper[Scale-space filtering](https://www.ijcai.org/Proceedings/83-2/Papers/091.pdf)  applied a whole spectrum of scales.\n",
    "He then plotted the zero-crossings Vs the scales in what he called a scale-space.\n",
    "He determined _stable_ features as ones that are stable over a range of scales.\n",
    "\n",
    "\n",
    "### $\\sigma$ and LoG\n",
    "Scale is about the size of the $\\sigma$ value we use in the LoG function.\n",
    "\n",
    "Different size $\\sigma$ will give local maxima at different positions and scales.\n",
    "\n",
    "Unlike Witkin, we are going to use the one best scale here, i.e. the one with the highest maxima.\n",
    "\n",
    "\n",
    "### Building a Scale-space\n",
    "All scales must be examined to identify scale invariant features.\n",
    "\n",
    "An efficient way is to compute the Laplacian Pyramid via a Difference of Gaussians (DoG).\n",
    "\n",
    "This was outlined in a previous lecture.\n",
    "\n",
    "\n",
    "\n",
    "### Difference of Gaussian Pyramid}\n",
    "\n",
    "![](images/DoGPyramid.png}\n",
    "\n",
    "David Lowe: [SIFT Paper](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)\n",
    "\n",
    "\n",
    "### How many scales?\n",
    "Lowe determined that the best number  of scales-per-octave was three (more than shown in the diagram).\n",
    "\n",
    "The [SIFT Paper](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) is a master class in how to find the best parameter values and is worth reading just for that.\n",
    "\n",
    "\n",
    "\n",
    "### Peak Detection\n",
    "\n",
    "\n",
    "How to detect the peak in the scale space?\n",
    "\n",
    "Compare pixel ($\\times$)  with the 26 pixels in current and adjacent scales (green).\n",
    "\n",
    "It is selected, only if it is larger than all of its 26 neighbours or smaller than all of them.\n",
    "\n",
    "There will be a large number of these extrema.\n",
    "\n",
    "Generally, extrema that are close together are unstable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](images/26pixels.png}\n",
    "David Lowe: SIFT Paper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Refining Results\n",
    "So we need to remove extrema that are poor candidates.\n",
    "\n",
    "First the candidates are tested against nearby data, for location, scale and ratio of principle curvatures.\n",
    "\n",
    "If points have low contrast or are poorly localised along an edge, they are rejected.\\\\\n",
    "To do this they use a Taylor expansion up to quadratic terms of the scale-space function $D(x,y,\\sigma)$ at the sample point.\n",
    "\n",
    "\\begin{equation}\n",
    "\tD(x) = D + \\frac{\\partial D^{\\top}}{\\partial x}x + \\frac{1}{2}x^{\\top}\\frac{\\partial^2D}{\\partial x^2}x\n",
    "\\end{equation}\n",
    "\n",
    "The extremum, $\\hat{x}$, is determined by taking the derivative of this function with respect to $x$ and setting to zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\hat{x}=-\\frac{\\partial^2D^{-1}}{\\partial x^2}\\frac{\\partial D}{\\partial x}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "If the offset $\\hat{x}$ is more than 0.5 in any dimension it means the extremum is closer to a different sample point.\n",
    "\n",
    "In which case it is moved and the interpolation re-performed.\n",
    "\n",
    "The final offset $\\hat{x}$ is added to its sample point to get the interpolated estimate for the location of the extremum.\n",
    "\n",
    "The function value at the extremum, $D(\\hat{x})$ is used for rejecting unstable extremum with low contrast.\n",
    "It is given by \n",
    "\\begin{equation}\n",
    "\tD(\\hat{x}) = D +\\frac{1}{2}\\frac{\\partial D^{\\top}}{\\partial x}\\hat{x}\n",
    "\\end{equation}\n",
    "\n",
    "Lowe used a value of $<0.03$ as the threshold for discarding an extremum, assuming image pixel values are in the range $[0,1]$.\n",
    "\n",
    "\n",
    "\n",
    "### Extremum Removal\n",
    "\n",
    "\n",
    "\n",
    "![](images/ExtremumRemoval.png}\n",
    "\n",
    "Image Credit: David Lowe - SIFT Paper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Eliminating Edge Responses\n",
    "A poorly defined peak in the DoG will have a large principal curvature across the edge but a small one in the perpendicular direction.\n",
    "\n",
    "Principle curvature can be computed from a $2\\times2$ [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) $\\mathbf{H}$ computed at the position and scale of the keypoint. \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{H}=\\begin{bmatrix}\n",
    "D_{xx} & D_{xy}\\\\\n",
    "D_{xy} & D_{yy}\n",
    " \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The eigenvalues of $\\mathbf{H}$ are proportional to the principal curvatures of D. In the same way as with the structure matrix $\\textbf{M}$ in Harris-Stephens we can determine something about the two eigen values without actually calculating them.\n",
    "\n",
    "\\begin{equation}\n",
    "\tTr(\\mathbf{H}) = \\lambda_1+\\lambda_2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\tDet(\\mathbf{H}) = \\lambda_1\\lambda_2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The important test here is whether the eigen values are different from each other in size.\n",
    "If the ratio is above some threshold $r$ then they should be eliminated.\n",
    "Lowe uses a value of $r=10$\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\frac{Tr(\\mathbf{H}^2)}{Det(\\mathbf{H})}<\\frac{(r+1)^2}{r}\n",
    "\\end{equation}\n",
    "\n",
    "This is quite fast to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89fc09-2680-4d8e-a43d-3669871b5755",
   "metadata": {},
   "source": [
    "## Orientation Assignment\n",
    "We need to achieve rotation invariance of the key-points, and this means assigning a consistent orientation to each.\n",
    "\n",
    "The scale of the keypoint is used to select the Gaussian smoothed image, L, with the closest scale so that all computations are performed in a scale invariant manner.\n",
    "\n",
    "For each image sample, $L(x,y)$ at this scale, the gradient magnitude, $m(x,y)$, and orientation,  $\\theta(x,y)$ is pre-computed using pixel differences.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "m(x,y) = \\sqrt{(L(x+1,y) - L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta(x,y) = \\tan^{-1}\\left(\\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y) - L(x-1,y)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "An orientation histogram is formed from the gradient orientations of sample points within a region around the keypoint.\n",
    "\n",
    "The orientation histogram has 36 bins covering $360^o$ range of orienations.\n",
    "\n",
    "Each sample is added to the histogram after it is weighted by its gradient magnitude and by a gaussian-weighted circular window with a $\\sigma$ that is 1.5 times the scale of the keypoint.\n",
    "\n",
    "Peaks in the orientation histogram correspond to dominant directions of local gradients.\n",
    "The highest peak in the histogram is detected, and then any other local peak that is within 80% of the highest peak is also used to create a key point with that orientation.\n",
    "\n",
    "So for locations with multiple peaks of similar magnitude, there will be multiple keypoints created at the same location and scale but different orientations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Orientation Histogram\n",
    "Only about 15% of the points are assigned multiple orientations, but they contribute significantly to the stability of matching.\n",
    "\n",
    "Finally, a parabola is fit to the three histogram values closest to each peak to interpolate the peak position for better accuracy.\n",
    "\n",
    "The peak is important. It tells us the dominant orientation for the whole feature.\n",
    "\n",
    "Along with location and scale, this is the third piece of information that will be used as the anchor that the rest of our feature descriptor will be relative to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc858d6-db7b-4cfe-a9e5-061351310a25",
   "metadata": {},
   "source": [
    "## Keypoint Descriptors\n",
    "The previous operations have assigned an image location, scale, and orientation to each key point.\n",
    "\n",
    "These parameters impose a repeatable local 2D coordinate system in which to describe the local image region and therefore provide invariance to these parameters.\n",
    "\n",
    "The next step is to compute a descriptor for the local image region that is highly distinctive yet is as invariant as possible to remaining variations, such as a change in illumination or 3D viewpoint.\n",
    "\n",
    "\n",
    "### Feature Descriptor\n",
    "\n",
    "![](images/KeypointDescriptor.png)\n",
    "\n",
    "Gradients to keypoint descriptors\n",
    "\n",
    "### Orientation Histogram\n",
    "\n",
    "\n",
    "First, the image gradient magnitudes and orientations are sampled around the keypoint location, using the scale of the keypoint to select the Gaussian blur for the image.\n",
    "\n",
    "**To achieve orientation invariance, the coordinates of the descriptor and the gradient orientations are rotated relative to the keypoint orientation.**\n",
    "\n",
    "Each of the computed gradients are indicated with small arrows at each sample location in the figure.\n",
    "\n",
    "\n",
    "![](images/OrientationHistogram.png)\n",
    "\n",
    "Image Credit: Szeliski - Text Book\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A Gaussian weighting function with $\\sigma$ equal to half the width of the descriptor window is used to assign a weight to the magnitude of each sample point. This is the circle in the figure.\n",
    "\n",
    "This prevents sudden changes in the descriptor with small changes in the position of the window and gives less emphasis to gradients that are far from the center of the descriptor, as these are most affected by misregistration errors.\n",
    "\n",
    "\n",
    "\n",
    "The keypoint descriptor is shown on the right side of the figure. \n",
    "\n",
    "It allows for a significant shift in gradient positions by creating orientation histograms over $4\\times4$ sample regions. \n",
    "\n",
    "The figure shows eight directions for each orientation histogram, with the length of each arrow corresponding to the magnitude of that histogram entry.\n",
    "\n",
    "A gradient sample on the left can shift up to four sample positions while contributing to the same histogram on the right, thereby allowing for larger positional shifts.\n",
    "\n",
    "\n",
    "\n",
    "![](images/KeypointDescriptor.png)\n",
    "\n",
    "Gradients to keypoint descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbe04b-988c-4828-94aa-b6b93cfdb8a0",
   "metadata": {},
   "source": [
    "## Example}\n",
    "\n",
    "\n",
    "![](images/sift_keypointsA.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb093f78-fba8-47cf-8968-49596bcfa8b1",
   "metadata": {},
   "source": [
    "## Boundary Affects\n",
    "It is important to avoid boundary effects in which the descriptor abruptly changes as a sample shifts smoothly from being in one histogram to another or from one orientation to another.\n",
    "\n",
    "Therefore, trilinear interpolation is used to distribute the value of each gradient sample into adjacent histogram bins.\n",
    "\n",
    "So, each entry in a bin is multiplied by a weight of $1-d$ for each dimension, where $d$ is the distance of the sample from the central value of the bin as measured in units of the histogram bin spacing.\n",
    "\n",
    "\n",
    "\n",
    "## Keypoint Descriptors\n",
    "\n",
    "The descriptor is formed from a vector containing the values of all the orientation histograms entries, corresponding to the lengths of the arrows on the right side of the figure.\n",
    "\n",
    "The figure shows a $2\\times2$ array of orientation histograms, whereas the experiments in the paper show that the best results are achieved with a $4\\times4$ array of histograms with eight orientation bins in each.\n",
    "\n",
    "So this leads to $4\\times4\\times8=128$ element feature vector for each keypoint.\n",
    "\n",
    "\n",
    "## Illumination\n",
    "Finally, the feature vector is modified to reduce the effects of illumination change.\n",
    "\n",
    "First, the vector is normalised to unit length.\n",
    "\n",
    "If a change in image contrast changes each pixel by multiplying it by a constant, this will multiply gradients by the same constant, so this constant change will be cancelled by the vector normalisation.\n",
    "\n",
    "A brightness change in which a constant is added to the image pixels will not affect the gradient values as they are computed from pixel differences.\n",
    "\n",
    "Therefore the descriptor is invariant to affine changes in illumination.\n",
    "\n",
    "##  Non-linear Illumination\n",
    "\n",
    "Non-linear illumination can cause a large change in relative magnitudes for some gradients but are less likely to affect the gradient orientations.\n",
    "\n",
    "Therefore, to reduce the influence of large gradient magnitudes, we threshold the values in the unit feature vector to each be no larger than 0.2, and then re-normalising to unit length.\n",
    "\n",
    "So matching the magnitudes for large gradients is no longer as important, instead the distribution of orientations has greater emphasis. \n",
    "\n",
    "The value of 0.2 was determined experimentally using images containing differing illuminations for the same 3D objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0d4b2-386b-4d40-99cb-f5a491f714fb",
   "metadata": {},
   "source": [
    "![\"HigherEd 4.0 is funded by the Human Capital Initiative Pillar 3. HCI Pillar 3 supports projects to enhance the innovation and agility in response to future skills needs\"](images/HCIFunding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5c5a9-181c-41e2-adc1-a1558d2b6d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
