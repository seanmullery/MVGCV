{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917aced3",
   "metadata": {},
   "source": [
    "# Simple-Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d8b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# 2 lines below for html export\n",
    "import plotly.io as pio\n",
    "pio.renderers.default ='notebook'\n",
    "\n",
    "# 2 lines below for PDF export\n",
    "#!pip install Pyppeteer\n",
    "#!pyppeteer-install\n",
    "\n",
    "#This is a quick github test to see if it is working\n",
    "\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1d9c4",
   "metadata": {},
   "source": [
    "## Links and Acknowledgements\n",
    "\n",
    "[Mubarak Shah, University of Central Florida (UCF), Youtube](https://www.youtube.com/watch?v=715uLCHt4jE&list=PLd3hlSJsX_ImKP68wfKZJVIPTd8Ie5u-9)\n",
    "\n",
    "[James, Hays Georgia Tech, Notes](https://www.cc.gatech.edu/~hays/compvision/)\n",
    "\n",
    "[Richard Szeliski, Computer Vision, Text Book](https://www.amazon.co.uk/Computer-Vision-Algorithms-Applications-Science/dp/1848829345/ref=sr_1_1?ie=UTF8&qid=1548753076&sr=8-1&keywords=richard+szeliski)\n",
    "\n",
    "[Tamal Bose, Digital Signal and Image Processing, Text Book](https://www.amazon.co.uk/Digital-Signal-Image-Processing-Tamal/dp/9812531122/ref=sr_1_1?ie=UTF8&qid=1548753160&sr=8-1&keywords=digital+signal+and+image+processing+tamal+bose)\n",
    "\n",
    "[David Jerison, MIT 18.01 Single Variable Calculus, Youtube](https://www.youtube.com/watch?v=7K1sB05pE0A&list=PL590CCC2BC5AF3BC1)\n",
    "\n",
    "[Gilbert Strang, MIT 18.06 Linear Algebra, Youtube](https://www.youtube.com/watch?v=ZK3O402wf1c&list=PLE7DDD91010BC51F8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d922ddb",
   "metadata": {},
   "source": [
    "## Edges\n",
    "\n",
    "Edges in images convey an enormous amount of information. \n",
    "\n",
    "A line drawing of an image contains most of the information you need. \n",
    "\n",
    "Adding grey tones, colour and texture should not be underestimated but edges tell us the most.\n",
    "They delineate the boundary between objects.\n",
    "\n",
    "They also show where an object changes. \n",
    "\n",
    "We can also see shadow boundaries which may help with determining positions in an image.\n",
    "\n",
    "Edge detection is well studied with incremental improvement being made from one algorithm to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f1dcf",
   "metadata": {},
   "source": [
    "## Types of edge\n",
    "\n",
    "To think about edges you need to think about a change in the brightness (intensity) of pixels.\n",
    "This could be a change from bright to dark or dark to bright. Below are four types of edge model. Low values are dark and high values are bright.\n",
    "\n",
    "!['The image illustrates four types of edges commonly identified in image processing, each accompanied by a corresponding graphical representation. \"Step\": Displayed is a horizontal line with a sudden vertical change, resembling a step. \"Roof\": Illustrated is a line that gradually ascends and then immediately descends, forming a shape similar to a roof. \"Ramp\": Showcased is a line that smoothly ascends, resembling a ramp. \"Spike\": Depicted is a line with a sharp and narrow peak, representing a spike.\n",
    "Each type of edge is a different graphical representation of intensity transitions that might be encountered in an image.'](images/TypesOfEdge.jpeg)\n",
    "\n",
    "Once we mention change of brightness you should immediately think gradient/derivative.\n",
    "The gradient will pop up again and again throughout your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88b6e6",
   "metadata": {},
   "source": [
    "## Roberts Cross\n",
    "Lawrence Roberts (1963)\n",
    " \n",
    "Approximated the gradient of an image through discrete differentiation by computing the sum of squares of the differences between diagonally adjacent pixels. \n",
    "Start with Image $I(x,y)$\n",
    "Convolve with the following two kernels to get $G_x(x,y)$ and $G_y(x,y)$\n",
    "\\begin{equation}\n",
    "\tG_x(x,y) = I(x,y)*\\begin{bmatrix}\n",
    "               1 & 0\\\\\n",
    "               0 & -1    \\\\\n",
    "            \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\tG_y(x,y) = I(x,y)*\\begin{bmatrix}\n",
    "               0 & 1\\\\\n",
    "               -1 & 0  \\\\  \n",
    "            \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The magnitude of the gradient is given by \n",
    "\\begin{equation}\n",
    "\t\\nabla I(x,y) = g(x,y) = \\sqrt{G_x^2+G_y^2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\theta(x,y) = \\tan^{-1}\\left(\\frac{G_y(x,y)}{G_x(x,y)}\\right)-\\frac{3\\pi}{4}\n",
    "\\end{equation}\n",
    "\n",
    "The angle $0^o$ corresponds to a vertical orientation such that the direction of maximum contrast from black to white runs from left to right in the image.\n",
    "Roberts cross is simple and very fast to execute.\n",
    "However it is rarely used now because it operates very poorly in the presence of noise.\n",
    "\n",
    "This is what our next operator tries to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46d94b",
   "metadata": {},
   "source": [
    "## Prewitt and Sobel\n",
    "It may seem counter intuitive but these operators use a Gaussian filter to smooth the image before looking for edges. \n",
    "\n",
    "The Gaussian filter is great at smoothing noise as the noise tends to be individual pixels that are very different in brightness to the pixels around it. \n",
    "However as this is also how we spot an edge the smoothing will have some effect on the edge too. \n",
    "\n",
    "The formula for the 2D Gaussian is \n",
    "\\begin{equation}\n",
    "\tg(x,y) = e^{-\\frac{(x^2+y^2)}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "As $\\sigma$ increases, more pixels are involved in the average. (e.g. more calculations)\n",
    "\n",
    "As $\\sigma$ increases, the imaged is blurred more.\n",
    "\n",
    "As $\\sigma$ increases, noise is more effectively suppressed, but so are legitimate signals i.e. edges.\n",
    "\n",
    "Both Prewitt and Sobel use the following basic idea.\n",
    "\n",
    "- Smooth the image to suppress noise.\n",
    "- Compute derivative in the $x$ and $y$ directions. (note, not diagonally like Roberts.\n",
    "- Find the gradient of the Magnitude\n",
    "- Threshold the gradient of the Magnitude.\n",
    "\n",
    "\n",
    "### Prewitt\n",
    "\n",
    "Prewitt used the following kernels which both smooths and gets the derivative in x and y respectively. We see in each case the $3\\times3$ and it's two 1D separated kernels:\n",
    "\n",
    "$ x\\to \\text{direction}$      $\\begin{bmatrix}\n",
    "1 & 0& -1\\\\\n",
    "1 & 0& -1\\\\\n",
    "1 & 0& -1\\\\\n",
    "\\end{bmatrix}$   $\\to$   $\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \n",
    "\\end{bmatrix}$     $\\begin{bmatrix}\n",
    "1 & 0& -1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$ y\\to \\text{direction}$ $\\begin{bmatrix}\n",
    "1 & 1& 1\\\\\n",
    "0 & 0& 0\\\\\n",
    "-1 & -1& -1\\\\\n",
    "\\end{bmatrix}$ $\\to$ $\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "-1 \n",
    "\\end{bmatrix}$ $\\begin{bmatrix}\n",
    "1 & 1& 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Sobel\n",
    "\n",
    "Sobel didn't add much, simply gave higher precedence to the center pixel when smoothing.\n",
    "\n",
    "$ x\\to \\text{direction}$ $\\begin{bmatrix}\n",
    "1 & 0& -1\\\\\n",
    "2 & 0& -2\\\\\n",
    "1 & 0& -1\\\\\n",
    "\\end{bmatrix}$ $\\to$ $\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "1 \n",
    "\\end{bmatrix}$ $\\begin{bmatrix}\n",
    "1 & 0& -1\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "$y\\to \\text{direction}$ $\\begin{bmatrix}\n",
    "1 & 2& 1\\\\\n",
    "0 & 0& 0\\\\\n",
    "-1 & -2& -1\\\\\n",
    "\\end{bmatrix}$ $\\to$ $\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "-1 \n",
    "\\end{bmatrix}$ $\\begin{bmatrix}\n",
    "1 & 2& 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "**Note: I'm being unfair to Sobel here. Quite a bit was added in the mathematical model and in the $5\\times 5$ or $7\\times7$ this becomes much more obvious.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5eabad",
   "metadata": {},
   "source": [
    "## Marr-Hildreth\n",
    "\n",
    "David Marr and Ellen Hildreth gave the following method:\n",
    "\n",
    "- Smooth the image by Gaussian Filter $\\to S$\n",
    "- Apply Laplacian to $S \\to$ second order partial derivatives\n",
    "- Find zero crossings along each row and column $\\to$ max/min in first derivative is zero in second derivative.\n",
    "\n",
    "\n",
    "### Gaussian Smoothing \n",
    "\\begin{equation}\n",
    "\tS = g*I\n",
    "\\end{equation}\n",
    "\n",
    "where $g$ is \n",
    "\\begin{equation}\n",
    "\tg = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x^2+y^2)}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "### The Laplacian\n",
    "\\begin{equation}\n",
    "\t\\Delta^2S = \\frac{\\partial^2}{\\partial x^2}S+\\frac{\\partial^2}{\\partial y^2}S\n",
    "\\end{equation}\n",
    "\n",
    "But wait, here's a trick, because these are all linear operations we can just move the parenthesis\n",
    "\\begin{equation}\n",
    "\t\\Delta^2S = \\Delta^2(g*I) = (\\Delta^2g)*I\n",
    "\\end{equation}\n",
    "\n",
    "$\\Delta^2g$ is called the LoG (Laplacian of Gaussian).\n",
    "\\begin{equation}\n",
    "\t\\Delta^2g = \\frac{1}{\\sigma^3\\sqrt{2\\pi}}\\left(2-\\frac{(x^2+y^2)}{\\sigma^2}\\right)e^{-\\frac{(x^2+y^2)}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Finding Zero Crossings\n",
    "\n",
    "Four cases of zero crossings.\n",
    "\n",
    "- $\\{+,-\\}$\\\\\n",
    "- $\\{+,0,-\\}$\\\\\n",
    "- $\\{-,+\\}$\\\\\n",
    "- $\\{-,0,+\\}$\n",
    "\n",
    "Slope of zero crossing $\\{a,-b\\}$ is $|a+b|$\n",
    "\n",
    "To mark an edge\n",
    "\n",
    "- compute slope of zero-crossing\\\\\n",
    "- Apply a threshold to slope $\\to$ to mark the edges\\\\\n",
    "\n",
    "\n",
    " \n",
    "### Separability of Gaussian\n",
    "$h(x,y) = I(x,y)*g(x,y)$ requiring $n^2\\text{ multiplications}$\n",
    "\n",
    "$h(x,y) = I(x,y)*g_1(x)*g_2(y)$ requiring $2n\\text{ multiplications}$\n",
    "\n",
    "Note: values below have not been normalised.\n",
    "\n",
    "$ g_1 = \\begin{bmatrix}\n",
    "\\scriptstyle0.011 & \\scriptstyle0.13& \\scriptstyle0.6 &\\scriptstyle1.0&\\scriptstyle0.6&\\scriptstyle0.13&\\scriptstyle0.011\n",
    "\\end{bmatrix} $ $g_2 = \\begin{bmatrix}\n",
    "\\scriptstyle{0.011} \\\\\n",
    " \\scriptstyle{0.13}\\\\\n",
    " \\scriptstyle{0.6} \\\\\n",
    " \\scriptstyle{1.0}\\\\\n",
    " \\scriptstyle{0.6}\\\\\n",
    " \\scriptstyle{0.13}\\\\\n",
    " \\scriptstyle{0.011}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "### Separability of LoG\n",
    "\n",
    "LoG can also be separated into 1D convolutions but it requires $4\\times1D$ convolutions. However $4n$ is still less than $n^2$ if $n>4$\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\Delta^2S = \\Delta^2 (g*I) = (\\Delta^2*g)*I \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\Delta^2S = (I*g_{xx}(x))*g(y) + (I*g_{yy}(y))*g(x)\n",
    "\\end{equation}\n",
    "\n",
    "When using LoG we need to think about what $\\sigma$ size to use.\n",
    "\n",
    "Small sizes might produce too many edges but large size might miss some important edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3801a",
   "metadata": {},
   "source": [
    "## Canny Edge Detector\n",
    "\n",
    "John Canny - Purely mathematical technique.\n",
    "\n",
    "Uses Gradient of Gaussians.\n",
    "\n",
    "Canny's considerations on the Quality of an Edge\n",
    "\n",
    "- Robust to noise\n",
    "- Localization $\\to$ i.e. no shifting of the edge\n",
    "- Just the right number of responses.\n",
    "\n",
    "### The attributes of Canny\n",
    "\n",
    "- Criterion 1: Good Detection: Minimise the probability of false positives, as well as false negatives.\n",
    "- Criterion 2: Good localisation: The edges detected must be as close as possible to the true edges.\n",
    "- Single response Constraint: The detector must return one point only for each edge point.\n",
    "\n",
    "### Steps in the Canny Edge Detector\n",
    "\n",
    "1. Smooth image with Gaussian Filter.\n",
    "2. Compute the derivative of the filtered image.\n",
    "3. Find magnitude and orientation of gradient (note the others so far did not look at orientation of gradients).\n",
    "4. Apply non-maximum supression. \n",
    "5. Apply Hysteresis Threshold.\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "$ S = I*g(x,y)$ where $ g(x,y) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{x^2 + y^2}{2\\sigma^2}}$\n",
    "\n",
    "### Derivative\n",
    "\\begin{equation}\n",
    "\t\\nabla S = \\nabla(g*I)=(\\nabla g)*I \n",
    "\t\\end{equation}\n",
    "    \n",
    "\\begin{equation} \\nabla g= \\begin{bmatrix}\n",
    "\\frac{\\partial g}{\\partial x} \\\\\n",
    "\\frac{\\partial g}{\\partial y}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "g_x  \\\\\n",
    "g_y \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\nabla S = \\begin{bmatrix}\n",
    "g_x \\\\\n",
    "g_y\n",
    "\\end{bmatrix}*I = \\begin{bmatrix}\n",
    "g_x * I \\\\\n",
    "g_y * I\n",
    "\\end{bmatrix}\n",
    " \\end{equation}\n",
    " \n",
    " \n",
    "### Gradient Magnitute and Direction (Orientation)\n",
    "$(S_x, S_y)$ Gradient Vector\\\\\n",
    "Magnitude\n",
    "\\begin{equation}\n",
    "\t\\sqrt{(S_x^2+S_y^2)}\n",
    "\\end{equation}\n",
    "Direction\n",
    "\\begin{equation}\n",
    "\t\\Theta = \\arctan \\left(\\frac{S_y}{S_x}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now that we know the direction of the gradient we can use it.\\\\\n",
    "We take a slice along the direction normal (i.e. at $90^o$) to the edge. \\\\Again we have to make decisions about how long this slice is and how close the next slice will be.\\\\\n",
    "Along each slice we must determine the pixel with the largest gradient magnitude. \\\\The hope is that each of the points we pick will form a line/curve.\n",
    "\n",
    "### Non-maximum Suppression\n",
    "\n",
    "\\begin{equation}\n",
    "    M(x,y) = \n",
    "            \\begin{cases}\n",
    "            |\\nabla S|(x,y) \\text{ if } |\\nabla S|(x,y)| > |\\nabla S|(x',y')|   \\text{ and}\\\\ |\\nabla S|(x,y)| > |\\nabla S|(x'',y'')|\\\\\n",
    "            0 \\text{ otherwise}      \n",
    "            \\end{cases}\n",
    "\\end{equation}\n",
    "Where $x'$ and $y'$ are the neighbours of $x$ along the normal direction to an edge.\n",
    "\n",
    "### Hysteresis Thresholding\n",
    "\n",
    "\n",
    "Set a high threshold and a low threshold.\n",
    "\n",
    "If a pixel is above high, include all its neighbours as long as they do not dip below the low threshold. Continue this until you run out of neighbours or you dip below the low threshold.\n",
    "\n",
    "Scan the image from left-right, top-bottom.\n",
    "If the gradient at a pixel is \n",
    "\n",
    "- above **High**, declare it as an _edge pixel_\n",
    "- below **Low**, declare it as a _non-edge-pixel_\n",
    "- between **Low** and **High**, consider its neighbours iteratively then mark it as an _edge pixel_ if it is connected to an  _edge pixel_ directly or via pixels between **Low** and **High**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29017a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a26963",
   "metadata": {},
   "source": [
    "### What is a neighbouring pixel?\n",
    "Connectedness.\n",
    "\n",
    "- Four Connected\n",
    "- Eight Connected \n",
    "- Six connected.\n",
    "    \n",
    "In the figure below the red square represents the current pixel while any green squares represent neighbours.\n",
    "    \n",
    "![\"An image displaying three side-by-side grids, each with a 3x3 configuration, framed by thick black lines. The left and right grids are titled 'Four Connected' and 'Eight Connected' respectively. The middle grid, titled 'Six Connected'. all three have the center square shaded red. The and the four edge-adjacent squares are shaded green for 'Four Connect'. For 'Six Connected' the top-left corner and bottom-left corner is also shaded green. For 'Eight Connected' all eight squares surrounding the red square are shaded green.\"](images/ConnectedNeighbours.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b0995",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Intuitively, computer vision is about seeing things in images. \n",
    "We tend to step up from the word \"things\" and call the things we see, \"Features\". \n",
    "\n",
    "Features should be describable. Not necessarily in English words but we must have some way of identifying a feature. \n",
    "\n",
    "Imagine you are going to meet someone and you have to agree a place which will not be confused with somewhere else. In Dublin this would be **under the clock at Eason's** or at **the Spire**. \n",
    "You don't say meet me on the footpath.\n",
    "\n",
    "\n",
    "\n",
    "One of the main reasons for requiring a good description of a feature is that we want to be able to recognise that same feature in another image taken from another position. If we do, find the same feature in more than one image we have what we call a **correspondence** (or a match) between the two images. This means that the feature description must be somewhat invariant to the angle it is imaged from and to the brightness.\n",
    "\n",
    "Feature detection and matching is essential to many Computer Vision applications. It allows us to stitch images together to give a wider view. It allows us to determine something about depth in a stereo image. We will also see later that it helps us to determine the position of the cameras that took the images. Very useful indeed.\n",
    "\n",
    "Take a look at the images below. What do you think would make a good feature?\n",
    "\n",
    "![The image depicts a large, artistic sculpture of a fish, specifically resembling a salmon, with its tail curved towards the sky. The sculpture is set in what appears to be a public space, likely a park or square, with street lamps and parked cars in the background indicating an urban setting. The sculpture is made of a mosaic of different shades, likely representing scales, and is perched atop a circular base of rough stones. The sky in the background is painted with hues of purple and orange, suggesting either dawn or dusk, and the overall lighting gives the scene a serene, almost surreal quality.](images/SalmonSculptureColour.png)\n",
    "\n",
    "\n",
    "In terms of images, features that are easy to localise are called key-point features or interest points and usually the patch of pixels surrounding the point location are used to describe them. \n",
    "\n",
    "Edges are also useful, particularly if the edge has an interesting shape that makes it recognisable apart from other edges. So orientation of edges is important. \n",
    "\n",
    "As edges are often formed by the boundaries between objects they can be useful for determining when one object is in front of another and what the distance is between them, if we see it from two known points of view. When one object blocks another from view we call this occlusion. \n",
    "\n",
    "We can also group edges together to form longer sequences in images. Lane markings to lane direction for example. These can also be used to find the vanishing point. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918108",
   "metadata": {},
   "source": [
    "### Points and Patches\n",
    "\n",
    "Two terms to get to know: **Sparse** and **Dense**. They mean exactly what you would expect. Sparse is relatively few (with big gaps in between) and dense is many (with small or no gap in between).\n",
    "\n",
    "For point correspondence, in the first instance it is acceptable and indeed desirable to have a sparse set of well described points in multiple images so that we can determine the maximum from these with the minimum calculation. \n",
    "\n",
    "\n",
    "More is not necessarily better here, it may be worse. Consider thousands of point correspondences and we want to use these to determine only a few unknown parameters. If many of them are incorrect they will be misleading. \n",
    "\n",
    "We only need enough points to give us enough independent equations in the unknowns to solve the problem. The smaller number usually means we will pick the best, the most likely to be correct and get close to a single correct answer for each of our unknown parameters.\n",
    "\n",
    "Key points have the advantage of being able to match even in the presence of occlusions and significant changes in orientation.\n",
    "\n",
    "There are two broad approaches for this.\n",
    "\n",
    "    1. Find points of interest in one image. With a description or a small patch of pixels go searching for each of these in the other image(s). This works well for small baseline i.e. when the distance between image view points is small.  This might be consecutive frames from a video sequence where only small changes have occurred. In the case of video this can be used for example to steady a shaky video. \n",
    "    \n",
    "\t2.  Find points of interest separately in each of the images independently. Then see if any of these match the points of interest in the other images. Useful where the baseline is large, e.g. stitching panoramas, wide baseline stereo or recognising objects from different views.\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ae586",
   "metadata": {},
   "source": [
    "### Finding correspondences}\n",
    "Three stages.\n",
    "\n",
    "\n",
    "\n",
    "\t1.  Detect the features of interest.\n",
    "\t2.  Describe the features you have found.\n",
    "\t3.  Match or track features.\n",
    "        - Match these with features in other images.\n",
    "        - Track Features: in the small baseline this is used to check only \n",
    "                    a small neighbourhood so that we can track movement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What makes a good feature?\n",
    "Well clear blue sky or plain walls etc. are very poor. \n",
    "\n",
    "- They look the same everywhere. \n",
    "- This also goes for tarmac/asphalt/pavement.\n",
    "\n",
    "\n",
    "\n",
    "Clearly single dots do not. They are hard to tell apart.\n",
    "\n",
    "\n",
    "What about an edge (line)? \n",
    "-  Well these are better but if we are tracking a line and the movement is parallel to the line direction then all points on the line look similar and we can only deal with end points. \n",
    "- One end point looks the same as another. E.g. the end of a lane marker looks like the end of another.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Corners are better. \n",
    "-   I.e. gradients in at least two significantly different orientations.\n",
    "-  These are the easiest to pin-point.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When looking for good features, a useful technique is that of **auto-correlation**. It works as follows:\n",
    "\n",
    "- Test a patch of image against a small region around it. \n",
    "-  If it correlates well with the positions around it then it must look like the positions around it and is therefore not a useful patch. \n",
    "-  On the other hand if it correlates poorly with the surrounding region it must be very different from them and look unique (at least within a small defined region).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\tE_{AC}(\\Delta\\vec{u})=\\sum_i w(\\vec{x}_i)[I_0(\\vec{x}_i+\\Delta\\vec{u}) - I_0(\\vec{x}_i)]^2\n",
    "\\end{equation}\n",
    "\n",
    "$\\vec{u}=(u,v)$ the displacement vector. \n",
    "We sum over each of the pixels in the patch of interest $\\vec{x}_i$. $w(\\vec{x})$ is a spatially varying weighting function, e.g. the weight may be high in the centre of the patch and gets lower the further from the centre we get. \n",
    "Or it could be a simple box where every pixel in the patch is treated the same. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note the section  $[I_0(\\vec{x}_i+\\Delta\\vec{u}) - I_0(\\vec{x}_i)]$. \n",
    "We are comparing (via subtraction) the same image ($I_0$) at the current pixel $\\vec{x}_i$ and the pixel $\\Delta\\vec{u}$ from $\\vec{x}_i$. \n",
    "The \n",
    "\\begin{equation}\n",
    "\tE_{AC}(\\Delta\\vec{u})=\\sum_i w(\\vec{x}_i)[I_0(\\vec{x}_i+\\Delta\\vec{u}) - I_0(\\vec{x}_i)]^2\n",
    "\\end{equation}\n",
    "will give a single value for the single position $\\Delta\\vec{u}$ away from $\\vec{x}$. \n",
    "We will have to calculate many $\\Delta\\vec{u}$s in order to see the correlation of the patch $\\vec{x}$ with it's neighbourhood.\n",
    "\n",
    "For this reason we can assume that it is an expensive operation.\n",
    "\n",
    "\n",
    " This led to much research to find ways to calculate good approximations of the autocorrelation surface. (Lucas \\& Kanade 1981; Shi \\& Tomasi 1994) use the first parts of a Taylor series expansion of the image function $I_0(\\vec{x}_i+\\Delta\\vec{u}) \\approx I_0(\\vec{x}_i) + \\nabla I_0(\\vec{x}_i)\\cdot \\Delta\\vec{u}$. \n",
    " \n",
    "This is a form of [Linear Approximation](https://www.youtube.com/watch?v=BSAA0akmPEU&list=PL590CCC2BC5AF3BC1&index=8+Maybe+give+them+one+of+my+videos \"YouTube Video on Linear Approximation\")\n",
    ". If you are not familiar with the idea make sure to look it up.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc378cc2-8c80-4aba-9b6a-191928ddacb5",
   "metadata": {},
   "source": [
    "### Auto-correlation}\n",
    "\n",
    "$E_{AC}(\\Delta\\vec {u}) \\approx \\sum_i w(\\vec {x_i})[I_0(\\vec {x}_i) + \\nabla I_0(\\vec {x}_i)\\cdot \\Delta\\vec {u} - I_0(\\vec {x}_i)]^2 $\n",
    "\n",
    "\\begin{equation}\n",
    "\t= \\sum_i w(\\vec {x_i})[\\nabla I_0(\\vec {x_i}) \\Delta\\vec {u}]^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\t= \\sum_i w(\\vec {x_i})[\\Delta\\vec {u}^{\\top}\\nabla I_0^{\\top}(\\vec {x_i})\\nabla I_0(\\vec {x_i}) \\Delta\\vec {u}]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\Delta\\vec {u}^{\\top} \\mathbf{A}\\Delta\\vec {u}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Quadratic Bowl}\n",
    "\n",
    "This is a 2D Quadratic form as shown in the figure below and in interactive cell that follows this one.\n",
    "\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "\t\\nabla I_0(\\vec {x_i})=\\left(\\frac{\\partial I_0}{\\partial x},\\frac{\\partial I_0}{\\partial y}\\right)(\\vec {x_i})\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A}= w * \\begin{bmatrix}\n",
    "I^2_x & I_xI_y  \\\\ \n",
    "I_xI_y & I^2_y \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![\"This image displays a three-dimensional surface plot of a paraboloid, a shape that resembles an upward-facing bowl. The x and y axes are ranged from -5 to 5 and are marked on the grid. The z-axis represents the value of the function z=x-squared + y-squared, and extends upward, indicating positive values. The surface is colored with a gradient that ranges from dark blue at the lowest point at the center, through shades of purple and red, to bright yellow at the peaks. The gradient provides a visual cue for the height of the surface, with yellow representing the highest points and dark blue indicating the lowest. The plot is rendered against a white background with a grid that aids in perceiving the curvature of the paraboloid.\"](images/2DQuadratic.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " If we have the following\n",
    "\\begin{equation}\n",
    "\t\\begin{bmatrix}\n",
    "               u & v\n",
    "\\end{bmatrix} \\mathbf{A}\\begin{bmatrix}\n",
    "               u    \\\\\n",
    "               v\n",
    "            \\end{bmatrix} = constant\n",
    "\\end{equation}\n",
    "\n",
    "Then each constant draws an ellipse. See interative cell below, and play around with it.\n",
    "\n",
    "\n",
    "\n",
    "### Uncertainty Ellipse}\n",
    "$\\mathbf{A}$ can be factorised into it's Eigen Vectors and Eigen Values.\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\mathbf{R}^{-1}\\begin{bmatrix}\n",
    "               \\lambda_1 & 0 \\\\\n",
    "               0 & \\lambda_2\n",
    "            \\end{bmatrix}\\mathbf{R}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\lambda_1$ and $\\lambda_2$ are the Eigen values and $\\mathbf{R}$ has the associated Eigen Vectors in its columns.\n",
    "\n",
    "You can see the eigen values and vectors (direction) in the interactive plot below.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4849fd5-dcdb-4e4c-9fe1-d93df42edeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2250d9076b4ac68e30ab72745eb6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='Elevation', max=90), IntSlider(value=60, description='A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Define a function to update the plot with both elevation and azimuth angles\n",
    "def update_plot(elev_angle, azim_angle, contour_height, ellipse_ratio):\n",
    "    \n",
    "    # Create grid and compute function\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = np.linspace(-10, 10, 100)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    #ellipse_ratio = 5\n",
    "    z = ellipse_ratio*x**2 + y**2  # updated function\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(x, y, z, cmap='viridis', alpha=0.5)\n",
    "    \n",
    "    # Eigenvectors scaled by the square root of the eigenvalues\n",
    "    # For the ellipse equation ellipse_ratio*x^2 + y^2 = z_val, the eigenvalues are ellipse_ratio and 1 (coefficients of the equation)\n",
    "    # The eigenvectors are aligned with the x and y axes, so we don't need to calculate them\n",
    "    eigenvalues = np.array([ellipse_ratio , 1])\n",
    "    sqrt_eigenvalues = np.sqrt(eigenvalues)\n",
    "    \n",
    "    # Plot the ellipses and annotate directions at z=20\n",
    "    z_val = contour_height\n",
    "    theta = np.linspace(0, 2 * np.pi, 100)\n",
    "    x_ellipse =  np.sqrt(z_val / eigenvalues[0]) * np.cos(theta)\n",
    "    y_ellipse =  np.sqrt(z_val / eigenvalues[1]) * np.sin(theta)\n",
    "    z_ellipse = np.full_like(theta, z_val)\n",
    "    ax.plot(x_ellipse, y_ellipse, z_ellipse, color='red', linewidth=3)\n",
    "   \n",
    "    # Annotate the directions of the fastest and slowest change\n",
    "    ax.quiver(0, 0, z_val, np.sqrt(z_val / eigenvalues[0]), 0, 0, color='blue', lw=5)\n",
    "    ax.quiver(0, 0, z_val, 0, np.sqrt(z_val / eigenvalues[1]), 0,  color='cyan', lw=5  )\n",
    "    \n",
    "    \n",
    "    ax.text(np.sqrt(z_val / eigenvalues[0]), 0,z_val, r\"$\\sqrt{λ_1}$\", color='blue', fontsize=15)\n",
    "    ax.text(0, 3, np.sqrt(z_val / eigenvalues[1]), r\"$\\sqrt{λ_2}$\", color='cyan', fontsize=15)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax.view_init(elev=elev_angle, azim=azim_angle)\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders for elevation and azimuth\n",
    "elev_slider = widgets.IntSlider(min=0, max=90, step=1, value=30, description='Elevation')\n",
    "azim_slider = widgets.IntSlider(min=0, max=180, step=1, value=60, description='Azimuth')\n",
    "contour_slider = widgets.IntSlider(min=0, max=50, step=1, value=20, description='Contour')\n",
    "ellipse_slider = widgets.IntSlider(min=1, max=10, step=1, value=3, description='Ellipse Ratio')\n",
    "# Link the sliders to the update_plot function\n",
    "widgets.interactive(update_plot, elev_angle=elev_slider, azim_angle=azim_slider, contour_height=contour_slider, ellipse_ratio=ellipse_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa6e9a-5714-4134-b9b9-e6dc9889855b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
